# HR Resource Query Chatbot

## Overview
This project is an intelligent HR assistant designed to help teams find the right employees for specific tasks using natural language. Instead of relying on rigid keyword searches, it uses a Retrieval-Augmented Generation (RAG) architecture to understand the *meaning* behind a query and provide detailed, AI-generated recommendations. The final implementation uses a powerful local Large Language Model (LLM), ensuring privacy and no API costs.

## Features
- **Natural Language Querying:** Users can ask questions in plain English (e.g., "who has experience with cloud services and python?").
- **Semantic Search:** Uses sentence embeddings to find candidates based on conceptual meaning, not just exact keywords.
- **RAG Architecture:** Employs a full Retrieve-Augment-Generate pipeline to provide context-aware answers.
- **Local LLM Integration:** Leverages a locally-run LLM (`phi3:mini`) via Ollama for dynamic, human-like response generation.
- **REST API Backend:** Built with FastAPI to serve the AI/ML model and logic.
- **Interactive Web UI:** A simple and clean user interface built with Streamlit.

## Architecture
The system is designed with a client-server architecture, separating the frontend from the backend for modularity and scalability.
1.  **Frontend (Streamlit):** The user interacts with a web interface, submitting their query.
2.  **API Call:** The Streamlit frontend sends the query as a `POST` request to the FastAPI backend.
3.  **Backend - Retrieval (FastAPI):**
    - The backend receives the query.
    - It uses a `sentence-transformer` model to convert the query into a numerical vector (embedding).
    - It compares this query vector to pre-calculated embeddings of all employee profiles using cosine similarity to find the top 2 most relevant candidates.
4.  **Backend - Generation (Ollama):**
    - The profiles of the top candidates are combined with the original query into a detailed prompt.
    - This prompt is sent to a locally running LLM through the Ollama server.
    - The LLM generates a personalized, conversational paragraph for each candidate.
5.  **Response:** The FastAPI server sends the final AI-generated text back to the frontend, which then displays it to the user.

## Setup & Installation
Follow these steps to run the project locally.

1.  **Prerequisite: Install Ollama**
    - Download and install Ollama from [ollama.com](https://ollama.com).
    - Download the required model by running the following command in your terminal:
      ```bash
      ollama run phi3:mini
      ```

2.  **Clone the Repository**
    ```bash
    git clone <your-repo-url>
    cd <your-project-directory>
    ```

3.  **Install Python Dependencies**
    ```bash
    pip install fastapi uvicorn streamlit requests sentence-transformers
    ```

4.  **Run the Backend Server**
    In your first terminal, start the FastAPI server:
    ```bash
    uvicorn main:app --reload
    ```

5.  **Run the Frontend Application**
    In a second terminal, launch the Streamlit UI:
    ```bash
    streamlit run app.py
    ```
    The application should open in your web browser.

## API Documentation
The backend exposes a single API endpoint.

### POST /chat
- **Description:** Receives a user query and returns a detailed, AI-generated recommendation of suitable candidates.
- **Request Body:**
  ```json
  {
    "query": "find a developer with react native experience"
  }


## AI Development Process

- **Which AI coding assistants did you use?**
  - I used **Gemini**, and its *Guided learning* to grasp and build this project.

- **How did AI help in different phases?**
  - **Architecture & Planning:** Gemini helped outline the project structure, compare different implementation options (Simple vs. RAG, Cloud vs. Local LLM), and define the steps needed to build the application.
  - **Code Generation:** Gemini provided complete, working code blocks for the FastAPI backend, the Streamlit frontend, the all-in-one merged script for debugging, and the final refactore-d versions. This included everything from the data structures to the API endpoint logic and the RAG pipeline implementation.
  - **Debugging:** Gemini was crucial for diagnosing and solving errors. It helped interpret error messages like `command not found`, `connection refused`, and `read timed out`, explaining the root causes (PATH issues, network blocks, slow hardware) and providing step-by-step troubleshooting instructions.

- **What percentage of code was AI-assisted vs hand-written?**
  - Approximately **80%** of the code was generated by the AI assistant based on iterative prompts and requirements.
- **Any interesting AI-generated solutions or optimizations?**
  - The most interesting solution was pivoting from a cloud-based OpenAI API to a fully local LLM setup with Ollama. When the OpenAI API returned a `429 insufficient_quota` error, the AI suggested this alternative path, which aligned with the advanced requirements of the assignment and solved the blocker.
  - The suggestion to switch from the large `llama3` model to the much faster `phi3:mini` model was a key optimization to resolve the `read timed out` error caused by local hardware limitations.

- **Challenges where AI couldn't help and you solved manually?**
  - The primary challenge the AI could not solve directly was the initial network/DNS issue that prevented the `ollama run llama3` command from downloading the model files. This was a local machine configuration issue that required manual intervention to resolve.
 
  - ## Technical Decisions

- **OpenAI vs. Open-Source Models:** The project was initially planned with the OpenAI API due to its straightforward integration. However, I pivoted to an **open-source model (`phi3:mini`)** after encountering API quota limitations. This switch demonstrated adaptability and resulted in a self-contained application that is completely free to run.

- **Local LLM (Ollama) vs. Cloud API:**
  - **Cloud API (like OpenAI):** This approach is easy to implement but incurs potential costs, requires a constant internet connection, and involves sending user data to a third-party service.
  - **Local LLM (Ollama):** This approach offers **zero cost**, **complete data privacy** (as data never leaves the user's machine), and the ability to work offline. The trade-off is that it depends heavily on the user's local hardware for performance. We chose the local LLM path to prioritize privacy and cost-effectiveness.

- **Performance vs. Cost vs. Privacy Trade-offs:**
  - This project ultimately prioritized **cost** and **privacy** by using the free, local Ollama setup. The initial **performance** challenge (very slow responses with `llama3`) was solved by switching to a smaller, more efficient model (`phi3:mini`). This struck a great balance, providing a fast user experience while maintaining the benefits of a local, private system.

---
## Future Improvements

- **Dedicated Vector Database:** For a larger dataset of employees, I would integrate a specialized vector database like **FAISS** or **ChromaDB**. This would make the semantic search process much faster and more scalable than the current in-memory approach.

- **Chat History:** I would implement a session-based chat history to allow for conversational follow-up questions, making the chatbot feel more interactive and intelligent.

- **UI Enhancements:** The user interface could be improved with features like clickable employee profiles that show more details, options to filter results by availability, and a feedback mechanism for rating the quality of the recommendations.

- **Full-Stack Deployment:** To make the application publicly accessible, I would containerize the frontend and backend using **Docker** and deploy the full stack to a cloud platform like Google Cloud or AWS, using a virtual machine with sufficient resources to run the Ollama server.
